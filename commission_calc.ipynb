{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_add = \"202411-202412\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Velox Commission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_and_count_cancel_pairs(df):\n",
    "    # Separate buy/sell and cancel orders for easier processing\n",
    "    buy_sell_orders = df[df['Entry Type'].isin(['B', 'S'])]\n",
    "    cancel_orders = df[df['Entry Type'].isin(['BXL', 'SXL'])]\n",
    "    \n",
    "    # Create dictionaries to store buy/sell orders and their corresponding cancels\n",
    "    orders_dict = {}\n",
    "    matched_pairs = []\n",
    "    unmatched_rows = df.copy()\n",
    "\n",
    "    # Iterate over buy/sell orders to populate the orders_dict\n",
    "    for _, row in buy_sell_orders.iterrows():\n",
    "        #print(row.name)\n",
    "        key = (row['ACC#'], row['Symbol'], row['Entry Type'])\n",
    "        if key not in orders_dict:\n",
    "            orders_dict[key] = []\n",
    "        orders_dict[key].append((row['Qty'], row['Gross Amount'], row['Trade Date'], row.name))\n",
    "\n",
    "    # Iterate over cancel orders to find matching buy/sell orders\n",
    "    for _, row in cancel_orders.iterrows():\n",
    "        key = (row['ACC#'], row['Symbol'], 'B' if row['Entry Type'] == 'BXL' else 'S')\n",
    "        if key in orders_dict:\n",
    "            for i, (shares, amount, date, index) in enumerate(orders_dict[key]):\n",
    "                # Check if the shares and amount are additive inverses\n",
    "                if shares == -row['Qty'] and amount == -row['Gross Amount']:\n",
    "                    # Found a matching pair, remove it from orders_dict\n",
    "                    matched_pairs.append(df.loc[[index, row.name]])\n",
    "                    orders_dict[key].pop(i) \n",
    "                    unmatched_rows.drop(index=[index, row.name], inplace=True)\n",
    "                    break\n",
    "\n",
    "    # Create a dataframe containing all matched pairs\n",
    "    matched_pairs_df = pd.concat(matched_pairs) if matched_pairs else pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    # Return the count of pairs, the matched pairs dataframe, and the dataframe without the pairs\n",
    "    cancel_count = len(matched_pairs) // 2\n",
    "    return cancel_count, matched_pairs_df, unmatched_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\info\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_45244\\3470408429.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unmatched = pd.concat([unmatched, pairs_single], axis=0)\n",
      "C:\\Users\\info\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_45244\\3470408429.py:31: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  unmatched.fillna(\"\")\n",
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_45244\\3470408429.py:31: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  unmatched.fillna(\"\")\n",
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_45244\\3470408429.py:31: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  unmatched.fillna(\"\")\n",
      "C:\\Users\\info\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_45244\\3470408429.py:31: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  unmatched.fillna(\"\")\n",
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_45244\\3470408429.py:31: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  unmatched.fillna(\"\")\n",
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_45244\\3470408429.py:31: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  unmatched.fillna(\"\")\n"
     ]
    }
   ],
   "source": [
    "list = [\"93\", \"98\", \"85\"]\n",
    "out_dict = {}\n",
    "\n",
    "for rep in list:\n",
    "    velox = pd.read_excel(f\"namelist/commission/{date_add}/velox_comm/velox_{rep}.xlsx\")[[\n",
    "        \"ACC#\", \"Trade Date\", \"Settle Date\", \"Entry Type\", \"Symbol\", \"Qty\", \"Price\", \"Gross Amount\", \"Net Amount\", \"Comm\"\n",
    "    ]]\n",
    "    velox = velox[velox[\"Comm\"]!=0]\n",
    "    count, pairs, unmatched = filter_and_count_cancel_pairs(velox)\n",
    "    \n",
    "    # for any regular trade, the commission deduct is $5, for any pair it is $10\n",
    "    unmatched[\"Comm Deduct\"] = 5\n",
    "    pairs[\"Comm Deduct\"] = 10\n",
    "    unmatched[\"Net Commission\"] = pd.NA\n",
    "    \n",
    "    # Calculate commission\n",
    "    pairs_single = pairs[pairs[\"Entry Type\"].isin([\"SXL\",\"BXL\"])] # pairs commission only goes once\n",
    "    unmatched = pd.concat([unmatched, pairs_single], axis=0)\n",
    "    commission = sum(unmatched[\"Comm\"])\n",
    "    deduct = sum(unmatched[\"Comm Deduct\"])\n",
    "    net_comm = abs((commission+deduct)*0.6)\n",
    "\n",
    "    # Put the 3 values to the dataframe\n",
    "    unmatched = unmatched.reset_index()\n",
    "    pairs_single = pairs_single.reset_index()\n",
    "    pairs = pairs.reset_index()\n",
    "    tail = len(unmatched)\n",
    "    for i,j in [(commission, \"Comm\"), (deduct, \"Comm Deduct\"), (net_comm, \"Net Commission\")]:\n",
    "        #print(i,j)\n",
    "        unmatched.loc[tail, j] = i\n",
    "        unmatched.fillna(\"\")\n",
    "\n",
    "    out_dict[rep] = [count, pairs, unmatched]\n",
    "\n",
    "    pairs.to_csv(f\"namelist/commission/{date_add}/velox_comm/velox_comm_filtered/velox_{rep}_pairs.csv\")\n",
    "    unmatched.to_csv(f\"namelist/commission/{date_add}/velox_comm/velox_comm_filtered/velox_{rep}_unmatched.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curv Commission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_at_tail(df, name_val):\n",
    "    tail = len(df)\n",
    "    for name, value in name_val:\n",
    "        df[name] = pd.NA\n",
    "        df.loc[tail, name] = value\n",
    "    return df\n",
    "\n",
    "def clean_cash(value):\n",
    "    # Remove dollar signs and commas\n",
    "    value = value.replace('$', '').replace(',', '')\n",
    "    # Handle parentheses for negative values\n",
    "    if '(' in value and ')' in str(value):\n",
    "        value = '-' + value.replace('(', '').replace(')', '')\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "curv_rep_all = [\"85\", \"93\", \"98\"]\n",
    "special_rep = [\"85\"]\n",
    "special_acc = [\"AC9900022\", \"AC9900182\"]\n",
    "\n",
    "col_names = [\n",
    "    \"Account No\", \"Account Name\", \"Rep\",\n",
    "    \"Side\", \"Eff Trade Date\", \"Symbol\", \"Trade Date\", \"Qty\", \"Price\", \"Gross Amt\", \"Trns Id\",\n",
    "    \"Status\", \"Fees\", \"Net Amt\", \"Commission\", \"Contra Correspondent\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['curv_85.csv', 'curv_93.csv', 'curv_98.csv']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the folder containing the CSV files\n",
    "folder_path = f\"namelist/commission/{date_add}/curv_comm\"\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all CSV files\n",
    "dataframes = [pd.read_csv(os.path.join(folder_path, file)) for file in csv_files]\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Some data cleaning and format casting\n",
    "combined_df = combined_df[col_names]\n",
    "combined_df[\"Rep\"] = combined_df[\"Rep\"].fillna(0).astype(int).astype(str)\n",
    "combined_df[\"Qty\"] = combined_df[\"Qty\"].str.replace(\",\",\"\").fillna(0).astype(int)\n",
    "combined_df['Commission'] = combined_df['Commission'].astype(\"str\").apply(clean_cash).astype(\"float\")\n",
    "combined_df['Gross Amt'] = combined_df['Gross Amt'].astype(\"str\").apply(clean_cash)\n",
    "\n",
    "# kick out all ACSS Contra Correspondent, and delete all records with Quantity == 0\n",
    "combined_df = combined_df[combined_df[\"Contra Correspondent\"]!=\"ACSS\"]\n",
    "combined_df = combined_df[combined_df[\"Qty\"]!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter with Rep number\n",
    "out_dict = {}\n",
    "for rep in curv_rep_all:\n",
    "    curv_rep = combined_df[combined_df[\"Rep\"]==rep]\n",
    "    if rep not in special_rep:\n",
    "        out_dict[rep] = curv_rep\n",
    "    else:\n",
    "        special_acc_df = curv_rep[curv_rep[\"Account No\"].isin(special_acc)]\n",
    "        other_acc_df = curv_rep[~curv_rep[\"Account No\"].isin(special_acc)]\n",
    "        out_dict[rep] = {\n",
    "            \"special\": special_acc_df,\n",
    "            \"other\": other_acc_df\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filter_and_count_cancel_pairs_curv(df):\n",
    "    # Separate orders based on their status\n",
    "    cancel_orders = df[df['Status'] == 'Cancel']\n",
    "    executed_cxld_orders = df[df['Status'] == 'Executed CXLD']\n",
    "    correct_cxld_orders = df[df['Status'] == 'Correct CXLD']\n",
    "    \n",
    "    # Create dictionaries to store orders and their corresponding cancels\n",
    "    orders_dict = {}\n",
    "    matched_pairs = []\n",
    "    unmatched_rows = df.copy()\n",
    "\n",
    "    # Iterate over executed and correct canceled orders to populate the orders_dict\n",
    "    for _, row in pd.concat([executed_cxld_orders, correct_cxld_orders]).iterrows():\n",
    "        key = (row['Account No'], row['Symbol'], row['Status'])\n",
    "        if key not in orders_dict:\n",
    "            orders_dict[key] = []\n",
    "        orders_dict[key].append((row['Qty'], row['Gross Amt'], row['Trade Date'], row.name))\n",
    "\n",
    "    # Iterate over cancel orders to find matching executed or correct canceled orders\n",
    "    for _, row in cancel_orders.iterrows():\n",
    "        key_executed = (row['Account No'], row['Symbol'], 'Executed CXLD')\n",
    "        key_correct = (row['Account No'], row['Symbol'], 'Correct CXLD')\n",
    "        \n",
    "        # Check for matching pairs with Executed CXLD or Correct CXLD\n",
    "        for key in [key_executed, key_correct]:\n",
    "            if key in orders_dict:\n",
    "                for i, (shares, amount, date, index) in enumerate(orders_dict[key]):\n",
    "                    # Check if the shares and amount are additive inverses\n",
    "                    if shares == -row['Qty'] and amount == -row['Gross Amt']:\n",
    "                        # Found a matching pair, remove it from orders_dict\n",
    "                        matched_pairs.append(df.loc[[index, row.name]])\n",
    "                        orders_dict[key].pop(i)\n",
    "                        if row.name==10310:\n",
    "                            print(row)\n",
    "                            print(row.name)\n",
    "                            print(index)\n",
    "                        #unmatched_rows.drop(index=[index, row.name], inplace=True)\n",
    "                        break\n",
    "\n",
    "    # Create a dataframe containing all matched pairs\n",
    "    matched_pairs_df = pd.concat(matched_pairs) if matched_pairs else pd.DataFrame(columns=df.columns)\n",
    "    \n",
    "    # Return the count of pairs, the matched pairs dataframe, and the dataframe without the pairs\n",
    "    cancel_count = len(matched_pairs) // 2\n",
    "    return cancel_count, matched_pairs_df, unmatched_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curv_filter(df, pct):\n",
    "    if df['Commission'].dtype != float:\n",
    "        df['Commission'] = df['Commission'].apply(clean_cash)\n",
    "    if df['Gross Amt'].dtype != float:\n",
    "        df['Gross Amt'] = df['Gross Amt'].apply(clean_cash)\n",
    "\n",
    "    count, pairs, unmatched = filter_and_count_cancel_pairs_curv(df)\n",
    "    print(count, len(pairs))\n",
    "    total_shares = sum(df['Qty'])\n",
    "    gross_comm = sum(df['Commission'])\n",
    "\n",
    "    print(\"3 numbers\")\n",
    "    print(gross_comm, count*50, total_shares*0.007)\n",
    "    print(\"down\")\n",
    "    net_comm = (abs(gross_comm) - count*50 - abs(total_shares)*0.007)*pct\n",
    "\n",
    "    df.loc[len(df),\"Contra Correspondent\"] = gross_comm\n",
    "    df = add_at_tail(df, [\n",
    "        (\"Pair Cancelling Deduction\", count*50),\n",
    "        (\"To Curv and Spirit\", total_shares*0.007),\n",
    "        (\"Net Commission\", net_comm)\n",
    "    ])\n",
    "\n",
    "    #df = add_at_tail(df, count*50, \"Pair Cancelling Deduction\")\n",
    "    #df = add_at_tail(df, total_shares*0.007, \"To Curv and Spirit\")\n",
    "    #df = add_at_tail(df, net_comm, \"Net Commission\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "0 0\n",
      "3 numbers\n",
      "0 0 0.0\n",
      "donw\n",
      "0 0\n",
      "3 numbers\n",
      "-2783.47 0 -3252.732\n",
      "donw\n",
      "93\n",
      "0 0\n",
      "3 numbers\n",
      "0 0 0.0\n",
      "donw\n",
      "98\n",
      "0 0\n",
      "3 numbers\n",
      "0 0 0.0\n",
      "donw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_45244\\678064434.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[len(df),\"Contra Correspondent\"] = gross_comm\n",
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_45244\\678064434.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[len(df),\"Contra Correspondent\"] = gross_comm\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, key in enumerate(out_dict):\n",
    "    data = out_dict[key]\n",
    "    print(key)\n",
    "    if type(data) == dict: # special\n",
    "        special = data['special']\n",
    "        special = curv_filter(special, 0.6)\n",
    "        special.to_csv(f'{folder_path}/curv_comm_filtered/{key}_comm_special.csv')\n",
    "\n",
    "        other = data['other']\n",
    "        other = curv_filter(other, 0.1)\n",
    "        other.to_csv(f'{folder_path}/curv_comm_filtered/{key}_comm_other.csv')\n",
    "    else:\n",
    "        data = curv_filter(data, 0.6)\n",
    "        data.to_csv(f'{folder_path}/curv_comm_filtered/{key}_comm.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\info\\AppData\\Local\\Temp\\ipykernel_7596\\1276636170.py:1: DtypeWarning: Columns (28,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp = pd.read_csv(f\"{folder_path}/Activity_Oct 2024.csv\")\n"
     ]
    }
   ],
   "source": [
    "temp = pd.read_csv(f\"{folder_path}/Activity_Oct 2024.csv\")\n",
    "temp = temp[temp['Account No']==\"AC9900174\"][col_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv(f\"{folder_path}/HuaLi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
